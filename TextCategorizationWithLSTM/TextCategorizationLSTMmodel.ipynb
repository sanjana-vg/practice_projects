{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing sentences with Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from os import scandir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TextCategorizationUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business  -  3067\n",
      "entertainment  -  3012\n",
      "politics  -  3713\n",
      "sport  -  3202\n",
      "tech  -  3687\n"
     ]
    }
   ],
   "source": [
    "# Categories for data classification\n",
    "categories = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "# Reads text data (news articles) belonging to each of the 5 categories\n",
    "data = readTextFromFiles('datasets/bbc-fulltext/bbc/', categories, 350)\n",
    "\n",
    "# check the amount of text loaded for each category\n",
    "for category in data:\n",
    "    print(category, \" - \", len(data[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size -  7078 7078\n",
      "test set size -  1247 1247\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and test sets\n",
    "X_train, Y_train, X_test, Y_test = process_data(data, train_test_split = 0.85)\n",
    "print(\"training set size - \", len(X_train), len(Y_train))\n",
    "print(\"test set size - \", len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocubalary size:  400000\n",
      "Dimension of the embedding:  50\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load word embeddings that are pre-trained using a huge text corpus \n",
    "These word embeddings will be used as feature vectors for to represent words present in the datato enable the model to classify the words of similar meanings alike even if all the synomyns are not present in the training set\n",
    "GloVe embeddings that capture co-occurrances of words are used here\n",
    "''' \n",
    "embeddings, word_to_ind, ind_to_word, word_set = read_word_embeddings(\"pretrainedmodels/glove.6B.50d.txt\")\n",
    "\n",
    "print(\"Vocubalary size: \", len(word_set))\n",
    "n_emb = len(embeddings[ind_to_word[1]])\n",
    "print(\"Dimension of the embedding: \", n_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Converts category indices 0-5 to one-hot vectors\n",
    "params\n",
    "    category_to_index - maps category to respective index\n",
    "    Y - arrays of categories\n",
    "returns\n",
    "    one hot representation of Y\n",
    "'''\n",
    "def convert_categories_to_onehot(category_to_index, Y):\n",
    "    Y_cat = np.array([category_to_index[c] for c in Y])\n",
    "    Y_one_hot = np.eye(len(categories))[Y_cat]\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7078)\n",
      "(5, 1247)\n"
     ]
    }
   ],
   "source": [
    "category_to_index, index_to_category = get_category_index_map(categories)\n",
    "Y_train_one_hot = convert_categories_to_onehot(category_to_index, Y_train).T\n",
    "Y_test_one_hot = convert_categories_to_onehot(category_to_index, Y_test).T\n",
    "print(Y_train_one_hot.shape)\n",
    "print(Y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Converts a sentence to a vector of word embeddings\n",
    "params\n",
    "    sentence - text to be converted\n",
    "    embeddings - maps words to embeddings\n",
    "    word_set - set of words present in the vocubalary\n",
    "    n_emb - length of the feature vector\n",
    "    nt - number of words (time units) to be considered. words beyond nt will be ignored\n",
    "returns\n",
    "    feature vector representing the input sentence - dimension (nt, n_emb)\n",
    "'''\n",
    "def convert_sentence_to_embedding(sentence, embeddings, word_set, n_emb, nt):\n",
    "    words = sentence.split(' ')\n",
    "    emb = np.zeros((nt, n_emb))\n",
    "    for (ind, word) in enumerate(words):\n",
    "        if ind >= nt:\n",
    "            break\n",
    "        if word in word_set:\n",
    "            emb[ind,:] = embeddings[word]\n",
    "    return emb\n",
    "\n",
    "'''\n",
    "Converts an array of sentences to vectors of word embeddings\n",
    "params\n",
    "    sentence - array of text to be converted\n",
    "    embeddings - maps words to embeddings\n",
    "    word_set - set of words present in the vocubalary\n",
    "    n_emb - length of the feature vector\n",
    "    nt - number of words (time units) to be considered for each sentence. words beyond nt will be ignored\n",
    "returns\n",
    "    feature vector representing the input sentences - dimension (nt, n_emb, n_sentences)\n",
    "'''\n",
    "def convert_sentences_to_embeddings(sentences, embeddings, word_set, n_emb, nt):\n",
    "    emb = np.zeros((nt, n_emb, len(sentences)))\n",
    "    for (ind, sentence) in enumerate(sentences):\n",
    "        emb[:, :, ind] = convert_sentence_to_embedding(sentence, embeddings, word_set, n_emb, nt)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Forward propagation through a Long Short Term Memory Cell for one time unit\n",
    "params\n",
    "    xt - input feature vector for the word at time unit t\n",
    "    a_prev, c_prev - cell states representing the memory retained from the 0 to t-1 time units\n",
    "    params - model parameters (to be optimized)\n",
    "returns\n",
    "    a_out, c_out - output cell states\n",
    "    y_pred - prediction based on input from 0 to t time units\n",
    "'''\n",
    "def lstm_cell(xt, a_prev, c_prev, params):\n",
    "    cell_inp = tf.concat([a_prev, xt], axis=0)\n",
    "    \n",
    "    # forget gate\n",
    "    gf = sigmoid(tf.add(tf.matmul(params['Wf'], cell_inp), params['bf']))\n",
    "    \n",
    "    # update gate\n",
    "    gi = sigmoid(tf.add(tf.matmul(params['Wi'], cell_inp), params['bi']))\n",
    "    \n",
    "    #computing the next cell state\n",
    "    cdt = tf.tanh(tf.add(tf.matmul(params['Wc'], cell_inp), params['bc']))\n",
    "    c_out = tf.multiply(gf, c_prev) + tf.multiply(gi, cdt)\n",
    "    \n",
    "    # output gate\n",
    "    go = sigmoid(tf.add(tf.matmul(params['Wo'], cell_inp), params['bo']))\n",
    "    \n",
    "    # cell output\n",
    "    a_out = tf.multiply(go, tf.tanh(c_out))\n",
    "    y_pred = tf.add(tf.matmul(params['Wy'], a_out), params['by'])\n",
    "    \n",
    "    return a_out, c_out, y_pred\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + tf.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    e = tf.exp(z)\n",
    "    return e / tf.sum(e, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Forward propagation for the input sequence through Long Short Term Memory Cells\n",
    "params\n",
    "    X - input feature vector corresponsing to input sequence\n",
    "    params - model parameters (to be optimized)\n",
    "    n_a - length of the cell state (memory)\n",
    "    nt - number of time units considered for RNN\n",
    "returns\n",
    "    y_pred - prediction based on input from 0 to t time units\n",
    "'''\n",
    "def lstm_fwd_prop(X, params, n_a, mb, nt):\n",
    "    a_prev = np.zeros((n_a, mb))\n",
    "    c_prev = np.zeros((n_a, mb))\n",
    "    for t in range(0,nt):\n",
    "        a_prev, c_prev, y_pred = lstm_cell(tf.squeeze(tf.slice(X, [t,0,0],[1,-1,-1])), a_prev, c_prev, params)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Fetches placeholders and parameters required for LSTM model\n",
    "params\n",
    "    n_emb - length of the feature vector\n",
    "    n_a   - length of the cell state (memory)\n",
    "    n_y   - dimension of output (number of categories in this case)\n",
    "    mb    - size of the mini batch considered for each iteration of the gradient descent\n",
    "    n_t   - number of time units considered for RNN\n",
    "returns\n",
    "    xt - placeholder for input mini batch x\n",
    "    yt - placeholder for mini batch's output y\n",
    "    params - model parameters (to be optimized)\n",
    "'''\n",
    "def get_variables(n_emb, n_a, n_y, mb, n_t):\n",
    "    xt = tf.placeholder(shape=(n_t, n_emb, mb), dtype=\"float32\")\n",
    "    yt = tf.placeholder(shape=(n_y, mb), dtype=\"float32\")\n",
    "    n_inp = n_a + n_emb\n",
    "    tf.set_random_seed(0)\n",
    "    params={}\n",
    "    params[\"Wf\"] = tf.get_variable(name=\"Wf\", shape = (n_a, n_inp), initializer=tf.glorot_uniform_initializer(seed=0))\n",
    "    params[\"bf\"] = tf.get_variable(name=\"bf\", shape = (n_a, 1), initializer=tf.zeros_initializer())\n",
    "    params[\"Wi\"] = tf.get_variable(name=\"Wi\", shape = (n_a, n_inp), initializer=tf.glorot_uniform_initializer(seed=1))\n",
    "    params[\"bi\"] = tf.get_variable(name=\"bi\", shape = (n_a, 1), initializer=tf.zeros_initializer())\n",
    "    params[\"Wc\"] = tf.get_variable(name=\"Wc\", shape = (n_a, n_inp), initializer=tf.glorot_uniform_initializer(seed=2))\n",
    "    params[\"bc\"] = tf.get_variable(name=\"bc\", shape = (n_a, 1), initializer=tf.zeros_initializer())\n",
    "    params[\"Wo\"] = tf.get_variable(name=\"Wo\", shape = (n_a, n_inp), initializer=tf.glorot_uniform_initializer(seed=3))\n",
    "    params[\"bo\"] = tf.get_variable(name=\"bo\", shape = (n_a, 1), initializer=tf.zeros_initializer())\n",
    "    params[\"Wy\"] = tf.get_variable(name=\"Wy\", shape = (n_y, n_a), initializer=tf.glorot_uniform_initializer(seed=4))\n",
    "    params[\"by\"] = tf.get_variable(name=\"by\", shape = (n_y, 1), initializer=tf.zeros_initializer())\n",
    "    return xt, yt, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Computes categorical cost function for the Y predicted by the model\n",
    "params\n",
    "    y_pred - model's prediction\n",
    "    Y - expected output - labels from dataset\n",
    "returns\n",
    "    cost\n",
    "'''\n",
    "def compute_cost(y_pred, Y):\n",
    "    ent = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=Y)\n",
    "    cost = tf.reduce_mean(ent, axis = -1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Fetches the values of model parameters computed from the Tensorflow session\n",
    "'''\n",
    "def extract_param_values(sess, params, param_labels = ['Wf','bf', 'Wi', 'bi', 'Wc', 'bc', 'Wo', 'bo','Wy', 'by']):\n",
    "    param_vals = {}\n",
    "    for param_label in param_labels:\n",
    "        param_vals[param_label] = sess.run(params[param_label])\n",
    "    return param_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Runs mini batch gradient descent on the training data to optimize the parameters of the Recurrent Neural Network\n",
    "params\n",
    "    X_train, Y_train - dataset\n",
    "    learning_rate - hyperparameter for gradient descent optimizer\n",
    "    epochs  - number of epochs (number of times gradient descent is run on the entire training dataset)\n",
    "    embeddings - maps words to feature vectors\n",
    "    word_set - set of words in the vocabulary\n",
    "    n_a   - length of the cell state (memory)\n",
    "    n_t   - number of time units considered for RNN\n",
    "    mb    - size of the mini batch considered for each iteration of the gradient descent\n",
    "returns\n",
    "    parameters of the model\n",
    "'''\n",
    "def train_LSTM_model(X_train, Y_train, learning_rate, epochs, embeddings, word_set, n_a, nt, mb):\n",
    "    tf.reset_default_graph()\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    n_emb = len(embeddings[ind_to_word[1]])\n",
    "    ny, m = Y_train.shape\n",
    "    # preprocess data\n",
    "    X_emb = convert_sentences_to_embeddings(X_train, embeddings, word_set, n_emb, nt)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # build tensorflow computation graph to of the model and to compute cost\n",
    "        xt, yt, params = get_variables(n_emb, n_a, ny, mb, nt)\n",
    "        y_pred = lstm_fwd_prop(xt, params, n_a, mb,nt)\n",
    "        cost = compute_cost(tf.transpose(y_pred), tf.transpose(yt))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        seed = 0\n",
    "        \n",
    "        # run mini batch gradient descent\n",
    "        for i in range(0, epochs):\n",
    "            epoch_cost = 0\n",
    "            mini_batches = split_into_mini_batches(X_emb, Y_train, mb, seed) #to take advantage of vectorization for speeding up training\n",
    "            for (X_batch, Y_batch) in mini_batches:\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={xt:X_batch, yt:Y_batch})\n",
    "                epoch_cost += (c/len(mini_batches))\n",
    "            print(i, epoch_cost)\n",
    "            seed+=1\n",
    "        \n",
    "        #collect values of params\n",
    "        param_val = extract_param_values(sess, params)\n",
    "    return param_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.79861160255\n",
      "1 0.465986708873\n",
      "2 0.389155711121\n",
      "3 0.343277089273\n",
      "4 0.316566742191\n",
      "5 0.288139598238\n",
      "6 0.260910340347\n",
      "7 0.238651915033\n",
      "8 0.21465333836\n",
      "9 0.187985483082\n",
      "10 0.195392319657\n",
      "11 0.15529867673\n",
      "12 0.139644639591\n",
      "13 0.127415111123\n",
      "14 0.0985625788683\n",
      "15 0.092713656183\n",
      "16 0.0818260738066\n",
      "17 0.0717193161723\n",
      "18 0.0713806950251\n",
      "19 0.069797641425\n",
      "20 0.0466742935974\n",
      "21 0.076732538492\n",
      "22 0.103203131613\n",
      "23 0.0565559543577\n",
      "24 0.0305603635248\n",
      "25 0.0245327725369\n",
      "26 0.0211708109417\n",
      "27 0.042350919227\n",
      "28 0.0570166547838\n",
      "29 0.103865093109\n",
      "30 0.0537615505171\n",
      "31 0.0274125602476\n",
      "32 0.0214513543707\n",
      "33 0.0154494395586\n",
      "34 0.0220574334744\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "params = train_LSTM_model(X_train, Y_train_one_hot, learning_rate = 0.002, epochs = 35, embeddings= embeddings, \n",
    "                          word_set = word_set, n_a = 50, nt = 30, mb = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predits the categories for the given data using the trained model's parameters and computes accuracy\n",
    "params\n",
    "    X, Y - dataset\n",
    "    params - trained parameters of the model\n",
    "    embeddings - maps words to feature vectors\n",
    "    word_set - set of words in the vocabulary\n",
    "    n_a   - length of the cell state (memory)\n",
    "    n_t   - number of time units considered by RNN\n",
    "returns\n",
    "    accuracy\n",
    "'''\n",
    "def compute_accuracy(X, Y, params, embeddings, word_set, n_a, n_t):\n",
    "    n_emb = len(embeddings[ind_to_word[1]])\n",
    "    ny, m = Y.shape\n",
    "    # preprocess data\n",
    "    X_emb = convert_sentences_to_embeddings(X, embeddings, word_set, n_emb, n_t)\n",
    "    tf.reset_default_graph()\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        xt = tf.placeholder(shape=(n_t, n_emb, m), dtype=\"float32\")\n",
    "        yt = tf.placeholder(shape=(ny, m), dtype=\"float32\")\n",
    "        \n",
    "        # run forward propagation through the model\n",
    "        y_pred = lstm_fwd_prop(xt, params, n_a, m,n_t)\n",
    "        \n",
    "        # predict the categories with Softmax layer\n",
    "        stm = tf.nn.softmax(logits=tf.transpose(y_pred))\n",
    "        y_out = sess.run(stm, feed_dict={xt:X_emb})  \n",
    "\n",
    "        total_correct = np.sum(np.argmax(y_out, axis = 1) == np.argmax(Y.T, axis = 1))\n",
    "\n",
    "    return total_correct /m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995478948856\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy on training set\n",
    "train_acc = compute_accuracy(X_train, Y_train_one_hot, params, embeddings, word_set, 50, 30)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913392141139\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy on the test dataset\n",
    "perm = np.random.permutation(Y_test_one_hot.shape[1])\n",
    "Y_test_sch = Y_test_one_hot[:,perm]\n",
    "X_test_sch = np.array(X_test)[perm]\n",
    "test_acc = compute_accuracy(X_test_sch, Y_test_sch, params, embeddings, word_set, 50, 30)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predits the categories for the given data using the trained model's parameters\n",
    "params\n",
    "    X - array of sentences to categorize\n",
    "    params - trained parameters of the model\n",
    "    embeddings - maps words to feature vectors\n",
    "    word_set - set of words in the vocabulary\n",
    "    index_to_category - maps softmax unit's index to categody label\n",
    "    n_a   - length of the cell state (memory)\n",
    "    n_t   - number of time units considered by RNN\n",
    "returns\n",
    "    array of predicted categories\n",
    "'''\n",
    "def predict_category(X, params, embeddings, word_set,index_to_category, n_a = 50, nt = 20):\n",
    "    n_emb = len(embeddings[ind_to_word[1]])\n",
    "    m = len(X)\n",
    "    ny = 5\n",
    "    X_emb = convert_sentences_to_embeddings(X, embeddings, word_set, n_emb, nt)\n",
    "    tf.reset_default_graph()\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        xt = tf.placeholder(shape=(nt, n_emb, m), dtype=\"float32\")\n",
    "        yt = tf.placeholder(shape=(ny, m), dtype=\"float32\")\n",
    "        \n",
    "        # run forward propagation through the model\n",
    "        y_pred = lstm_fwd_prop(xt, params, n_a, m,nt)\n",
    "        stm = tf.nn.softmax(logits=tf.transpose(y_pred))\n",
    "        y_out = sess.run(stm, feed_dict={xt:X_emb})\n",
    "\n",
    "        prediction = [index_to_category[x] for x in np.argmax(y_out, axis = 1)]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sport', 'entertainment', 'tech', 'tech', 'business', 'politics']\n"
     ]
    }
   ],
   "source": [
    "# Running the classifier on random sentences\n",
    "X_sample = [\"Badminton is my favourite game that I enjoy playing with my friends during weekends no matter who wins or loses\",\n",
    "           \"The melodious tune of the flute gives great peace of mind to the listener\",\n",
    "           \"With rapid advancements in equipments and devices, humans will be relieved from performing dangerous tasks manually\",\n",
    "           \"dedicated work will lead to success\", \"total revenue\", \"the act was amended through a bill that was passed\"]\n",
    "sample_result = predict_category(X_sample, params, embeddings, word_set,index_to_category)\n",
    "print(sample_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "References:\n",
    "1. Datasest of categorized news articles\n",
    "        D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006\n",
    "        http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\n",
    "2. Pre-trained word embeddings\n",
    "        Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. \"GloVe: Global Vectors for Word Representation\"\n",
    "        http://nlp.stanford.edu/data/glove.6B.zip\n",
    "3. LSTM cell implementation and the usage of word embeddings as feature vectors was inspired by \n",
    "        Sequence Models course by DeepLearning.ai https://www.coursera.org/learn/nlp-sequence-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
